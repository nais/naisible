- hosts: coreos
  gather_facts: False
  user: "{{ remote_user }}"
  pre_tasks:
  - name: Copy python interpreter to server
    local_action: command scp files/pypy-5.1.0-linux64.tar.bz2 {{ remote_user }}@{{ inventory_hostname }}:/home/{{ remote_user }}/
    tags:
    - skip_ansible_lint
  roles:
    - coreos_bootstrap

- hosts: localhost
  tasks:
  - name: Verifiy that facts have been gathered for all hosts in inventory
    assert:
      that:
        - ansible_play_hosts == ansible_play_hosts_all

#- hosts: workers:!storage
#  user: "{{ remote_user }}"
#  roles:
#  - bigip_pool

- hosts: all
  user: "{{ remote_user }}"
  become: yes
  roles:
  -  { role: proxy, when: nais_http_proxy is defined }
  tasks:
  - name: Add systemd unit to fix issue 32728 (symlinks in /sys)
    copy:
      src: files/kube-issue-32728-fix.service
      dest: /etc/systemd/system/kube-issue-32728-fix.service
      owner: root
      group: root
      mode: 0644

  - name: Enable the fix for 32728 during boot
    systemd:
      daemon_reload: yes
      name: kube-issue-32728-fix.service
      state: started
      enabled: yes

- hosts: masters:workers
  user: "{{ remote_user }}"
  become: yes
  roles:
  - k8s_common
  tasks:
  - name: Copy .bashrc
    copy:
      src: files/bashrc
      dest: /root/.bashrc
  - name: Set nais puppet facts
    template:
      src: templates/nais-facts.txt.j2
      dest: /etc/puppetlabs/facter/facts.d/nais-facts.txt
    when: puppet_agent_rpm_check.rc == 0

- hosts: masters
  user: "{{ remote_user }}"
  roles:
  - fetch_existing_certificates

- hosts: localhost
  roles:
  - create_certificates

- hosts: etcd
  user: "{{ remote_user }}"
  become: yes
  roles:
    - etcd

- hosts: etcd[0]
  user: "{{ remote_user }}"
  become: yes
  tasks:
  - name: Check if flannel config exists (OK if this fails)
    command: "{{ install_dir }}/bin/etcdctl ls /nais/network/config"
    environment:
      - ETCDCTL_API: 2
    register: flannelconfig
    ignore_errors: yes
    changed_when: false

  - name: Set flannel configuration in etcd
    shell: '"{{ install_dir }}"/bin/etcdctl set /nais/network/config  "{ \"Network\": \"{{ pod_network_cidr }}\", \"SubnetLen\": 23, \"Backend\": { \"Type\": \"vxlan\" }}"'
    environment:
      - ETCDCTL_API: 2
    when: flannelconfig.rc != 0

# Configure kubernetes Master node
- hosts: masters
  user: "{{ remote_user }}"
  become: yes
  tasks:
  - name: Copy certificates
    copy:
      src: "target/pki/{{ item }}"
      dest: /etc/kubernetes/pki/
    with_items:
      - admin.pem
      - admin-key.pem
      - ca.pem
      - ca-key.pem
      - kube-apiserver-server-key.pem
      - kube-apiserver-server.pem
      - sa.key
      - sa.pub
      - traefik.pem
      - traefik-key.pem

- hosts: masters:workers
  user: "{{ remote_user }}"
  become: yes
  tasks:

  - name: Download flannel binaries # using curl as get_url gave protocol error (most likely caused by internal webproxy)
    environment: "{{ proxy_env }}"
    shell: curl -L --insecure https://github.com/coreos/flannel/releases/download/v{{flannel_version}}/flannel-v{{flannel_version}}-linux-amd64.tar.gz | tar xzf - -C /tmp flanneld mk-docker-opts.sh && \
           mv /tmp/flanneld {{ install_dir }}/bin/flanneld-{{flannel_version}} && \
           mv /tmp/mk-docker-opts.sh {{ install_dir }}/libexec
    args:
      creates: "{{ install_dir }}/bin/flanneld-{{flannel_version}}"
    tags:
      - skip_ansible_lint

  - name: Symlink flannel
    file:
      src: "{{ install_dir }}/bin/flanneld-{{flannel_version}}"
      dest: "{{ install_dir }}/bin/flanneld"
      state: link

  - name: Copy flannel service file
    template:
      src: templates/flannel.service.j2
      dest: /etc/systemd/system/flannel.service
    notify:
      - restart_flannel

  - name: Enable flannel
    systemd:
      daemon_reload: yes
      name: flannel.service
      state: started
      enabled: yes

  - name: Ensure /var/lib/docker exists
    file:
      path: /var/lib/docker
      state: directory

  - name: Force delete docker_opts.env
    file:
      path: /var/lib/docker/docker_opts.env
      state: absent

  - name: Make flannel configuration for docker
    command: "{{ install_dir }}/libexec/mk-docker-opts.sh -k DOCKER_OPTS -f /run/flannel/subnet.env -d /var/lib/docker/docker_opts.env"
    changed_when: false

  - name: Copy docker service file
    copy:
      src: files/docker.service
      dest: /etc/systemd/system/docker.service
    notify:
      - restart_docker

  - name: Setup docker daemon
    copy:
      src: files/daemon.json
      dest: /etc/docker/daemon.json
    notify:
      - restart_docker

  - name: Start and enable docker
    systemd:
      daemon_reload: yes
      name: docker.service
      state: started
      enabled: yes

  - name: Ensure /root/.docker exists
    file:
      path: /root/.docker
      state: directory

  - name: Add docker credentials if defined
    template:
      src: templates/docker-config.json.j2
      dest: /root/.docker/config.json
    when: docker_repo_url is defined

  - name: Disable swap automount
    mount:
      path: swap
      state: absent

  - name: Disable swap
    command: swapoff -a
    changed_when: false

  - name: Ensure rbd kernel module is present
    modprobe:
      name: rbd
      state: present
    when: ansible_distribution == 'RedHat'

  - name: Load rbd kernel module on boot
    copy:
      src: files/rbd.modules
      dest: /etc/sysconfig/modules/rbd.modules
      mode: 0755
    when: ansible_distribution == 'RedHat'

  - name: Download kubernetes node binaries # using curl as get_url gave protocol error (most likely caused by internal webproxy)
    environment: "{{ proxy_env }}"
    shell: curl -L --insecure https://dl.k8s.io/v{{ k8s_version }}/kubernetes-node-linux-amd64.tar.gz | \
           tar xzf - -C /tmp kubernetes/node/bin/kubectl kubernetes/node/bin/kube-proxy kubernetes/node/bin/kubelet && \
           mv /tmp/kubernetes/node/bin/kube-proxy {{ install_dir }}/bin/kube-proxy-{{ k8s_version }} && \
           mv /tmp/kubernetes/node/bin/kubectl {{ install_dir }}/bin/kubectl-{{ k8s_version }} && \
           mv /tmp/kubernetes/node/bin/kubelet {{ install_dir }}/bin/kubelet-{{ k8s_version }}
    args:
      creates: "{{ install_dir }}/bin/kubelet-{{k8s_version}}"
    tags:
      - dev
      - skip_ansible_lint

  - name: Symlink kubernetes node binaries
    file:
      src: "{{ install_dir }}/bin/{{ item }}-{{ k8s_version }}"
      dest: "{{ install_dir }}/bin/{{ item }}"
      state: link
      force: yes
    with_items:
      - kubectl
      - kube-proxy
      - kubelet
    notify:
      - restart_kubelet
      - restart_kubeproxy

  - name: Make kubernetes node binaries executable
    file:
      path: "{{ install_dir }}/bin/{{ item }}-{{ k8s_versionÂ }}"
      mode: 0755
    with_items:
      - kubectl
      - kube-proxy
      - kubelet

  - name: Copy kube-proxy service file
    template:
      src: templates/kube-proxy.service.j2
      dest: /etc/systemd/system/kube-proxy.service
    notify:
      - restart_kubeproxy

  - name: Copy kube-proxy kubeconfig
    template:
      src: templates/kubeconfigs/kube-proxy.conf.j2
      dest: /etc/kubernetes/kubeconfigs/kube-proxy.conf
    notify:
      - restart_kubeproxy

  - name: Enable kube-proxy
    systemd:
      daemon_reload: yes
      name: kube-proxy.service
      state: started
      enabled: yes

  - name: Clean up /etc/hosts after puppet
    shell: /bin/sed -i "s/$(/sbin/ifconfig docker0 | grep inet | awk '{print $2}')/$(/sbin/ifconfig $(ls /sys/class/net/ | grep en) | grep inet | awk '{print $2}')/g" /etc/hosts
    when: puppet_agent_rpm_check.rc == 0
    tags:
      - skip_ansible_lint

  - name: Set kernel parameters
    sysctl:
      name: "{{ item.kname }}"
      value: "{{ item.kvalue }}"
      state: present
      sysctl_file: /etc/sysctl.d/60-sysctl.conf
      sysctl_set: yes
      reload: yes
    with_items:
      - { kname: 'fs.inotify.max_user_watches', kvalue: '16384' }

  handlers:
    - name: restart_docker
      systemd:
        daemon_reload: yes
        name: docker.service
        state: restarted

    - name: restart_kubelet
      systemd:
        daemon_reload: yes
        name: kubelet.service
        state: restarted
      # on new nodes kublet will not yet be install, ignore errors
      ignore_errors: yes

    - name: restart_flannel
      systemd:
        daemon_reload: yes
        name: flannel.service
        state: restarted

    - name: restart_kubeproxy
      systemd:
        daemon_reload: yes
        name: kube-proxy.service
        state: restarted

# Start control plane
- hosts: masters
  user: "{{ remote_user }}"
  become: yes
  tasks:

  - name: Copy kubelet service manifest
    template:
      src: templates/master-kubelet.service.j2
      dest: /etc/systemd/system/kubelet.service
    notify:
      - restart_kubelet

  - name: Copy kubelet kubeconfig
    template:
      src: templates/kubeconfigs/master-kubelet.conf.j2
      dest: /etc/kubernetes/kubeconfigs/kubelet.conf
    notify:
      - restart_kubelet

  - name: Enable kubelet
    systemd:
      daemon_reload: yes
      name: kubelet.service
      state: started
      enabled: yes

  - name: Ensure kubectl config directory exists on master
    file:
      state: directory
      path: /root/.kube

  - name: Copy API server pod description
    copy:
      src: files/kubectl-config
      dest: /root/.kube/config

  - name: Ensure manifests directory exists on master
    file:
      state: directory
      path: /etc/kubernetes/manifests

  - name: Copy manifests and kubeconfigs
    template:
      src: "templates/{{ item }}.j2"
      dest: "/etc/kubernetes/{{ item }}"
    tags:
    - devel
    with_items:
      - kube-apiserver-audit-policy.yaml
      - manifests/kube-apiserver.yaml
      - manifests/kube-scheduler.yaml
      - manifests/kube-controller-manager.yaml
      - kubeconfigs/kube-scheduler.conf
      - kubeconfigs/kube-controller-manager.conf
    notify:
      - restart_docker

  handlers:
    - name: restart_docker
      systemd:
        daemon_reload: yes
        name: docker.service
        state: restarted

    - name: restart_kubelet
      systemd:
        daemon_reload: yes
        name: kubelet.service
        state: restarted

# Configure workers
- hosts: workers
  user: "{{ remote_user }}"
  become: yes
  tasks:

  - name: Copy certificates
    copy:
      src: "target/pki/{{ item }}"
      dest: /etc/kubernetes/pki/ # to ensure idempotency
    with_items:
      - admin.pem
      - admin-key.pem
      - ca.pem
    notify:
      - restart_kubelet
      - restart_kubeproxy

  - name: Copy Issuing Internal certificate
    copy:
      src: files/NAVIssuingCAIntern.crt
      dest: "{{ cert_dir }}/NAV_Issuing_Intern_CA{{ cert_postfix }}"
      force: no

  - name: Ensure certs.d directory exists worker nodes
    file:
      state: directory
      path: /etc/docker/certs.d/docker.adeo.no:5000/

  - name: Symlink Internal CA to docker
    file:
      src: "/etc/ssl/certs/NAV_Issuing_Intern_CA{{ cert_postfix }}"
      dest: /etc/docker/certs.d/docker.adeo.no:5000/ca.crt
      state: link
      force: yes

  - name: Copy kubelet service manifest
    template:
      src: templates/worker-kubelet.service.j2
      dest: /etc/systemd/system/kubelet.service
    notify:
      - restart_kubelet

  - name: Copy kubelet kubeconfig
    template:
      src: templates/kubeconfigs/worker-kubelet.conf.j2
      dest: /etc/kubernetes/kubeconfigs/kubelet.conf
    notify:
      - restart_kubelet

  - name: Enable kubelet
    systemd:
      daemon_reload: yes
      name: kubelet.service
      state: started
      enabled: yes

  handlers:
    - name: restart_kubeproxy
      systemd:
        daemon_reload: yes
        name: kube-proxy.service
        state: restarted

    - name: restart_kubelet
      systemd:
        daemon_reload: yes
        name: kubelet.service
        state: restarted

# Common setup for all nodes
- hosts: masters:workers:coreos:storage
  user: "{{ remote_user }}"
  become: yes

  tasks:
  - name: Delete cluster-admin.conf
    file:
      path: /etc/kubernetes/kubeconfigs/cluster-admin.conf
      state: absent
    changed_when: false

  - name: Generate cluster-admin kubeconfig
    shell: "{{ install_dir }}/bin/kubectl config set-cluster {{ cluster_name }} --server=https://{{ groups['masters'][0] }}:6443 --certificate-authority=/etc/kubernetes/pki/ca.pem --embed-certs=true --kubeconfig=/etc/kubernetes/kubeconfigs/cluster-admin.conf &&
           {{ install_dir }}/bin/kubectl config set-credentials {{ cluster_name }}-cluster-admin --client-certificate=/etc/kubernetes/pki/admin.pem --client-key=/etc/kubernetes/pki/admin-key.pem --embed-certs --kubeconfig=/etc/kubernetes/kubeconfigs/cluster-admin.conf &&
           {{ install_dir }}/bin/kubectl config set-context {{ cluster_name }} --cluster={{ cluster_name }} --user={{ cluster_name }}-cluster-admin --kubeconfig=/etc/kubernetes/kubeconfigs/cluster-admin.conf &&
           {{ install_dir }}/bin/kubectl config use-context {{ cluster_name }} --kubeconfig=/etc/kubernetes/kubeconfigs/cluster-admin.conf"
    changed_when: false

  - name: Wait for API server to become available
    wait_for:
      port: 6443
      host: "{{ groups['masters'][0] }}"
      delay: 15
      timeout: 240
    changed_when: false

  - name: Setup node taints
    command: "{{ install_dir }}/bin/kubectl --kubeconfig=/etc/kubernetes/kubeconfigs/cluster-admin.conf  --overwrite=true taint nodes {{ inventory_hostname }} {{ item }}"
    with_items: "{{ node_taints | default([]) }}"
    changed_when: false

  - name: Setup node labels
    command: "{{ install_dir }}/bin/kubectl --kubeconfig=/etc/kubernetes/kubeconfigs/cluster-admin.conf --overwrite=true label nodes {{ inventory_hostname }} {{ item }}"
    with_items: "{{ node_labels | default([]) }}"
    changed_when: false


- hosts: masters
  user: "{{ remote_user }}"
  become: yes

  tasks:
  - name: Ensure addons directory exists
    file:
      state: directory
      path: /etc/kubernetes/addons

  - name: Ensure roles directory exists
    file:
      state: directory
      path: /etc/kubernetes/roles

  - name: Copy kubernetes-addons manifest templates
    template:
      src: "templates/addons/{{ item }}.j2"
      dest: "/etc/kubernetes/addons/{{ item }}"
    with_items:
      - core-dns.yaml
      - traefik.yaml
      - heapster.yaml
      - nais.yaml
      - debug.yaml


  - name: Copy kubernetes clusterroles and clusterrolebindings
    copy:
      src: "templates/roles/{{ item }}.j2"
      dest: "/etc/kubernetes/roles/{{ item }}"
    with_items:
      - clusterroles.yaml
      - clusterrolebindings.yaml

  - name: Wait for API server to become available
    wait_for:
      port: 8080
      host: 127.0.0.1
      delay: 5
    changed_when: false

  - name: Create clusterrole and clusterrolebindings
    command: "{{ install_dir }}/bin/kubectl apply -f /etc/kubernetes/roles/{{ item }}"
    with_items:
    - clusterroles.yaml
    - clusterrolebindings.yaml
    changed_when: false

  - name: Create traefik tls secret
    command: "{{ install_dir }}/bin/kubectl -n kube-system create secret tls traefik-cert --key=/etc/kubernetes/pki/traefik-key.pem --cert=/etc/kubernetes/pki/traefik.pem"
    ignore_errors: yes
    changed_when: false

  - name: Create addons
    command: "{{ install_dir }}/bin/kubectl apply -f /etc/kubernetes/addons/{{ item }}"
    with_items:
      - core-dns.yaml
      - traefik.yaml
      - heapster.yaml
      - nais.yaml
      - debug.yaml
    changed_when: false

  - name: Download helm binaries # using curl as get_url gave protocol error (most likely caused by internal webproxy)
    environment: "{{ proxy_env }}"
    shell: curl -L --insecure https://storage.googleapis.com/kubernetes-helm/helm-v{{ helm_version }}-linux-amd64.tar.gz | \
           tar xzf - -C /tmp linux-amd64/helm && \
           mv /tmp/linux-amd64/helm {{ install_dir }}/bin/helm-{{ helm_version }}
    args:
      creates: "{{ install_dir }}/bin/helm-{{ helm_version }}"
    tags:
      - skip_ansible_lint

  - name: Symlink helm
    file:
      src: "{{ install_dir }}/bin/helm-{{ helm_version }}"
      dest: "{{ install_dir }}/bin/helm"
      state: link

  - name: Setup helm service account
    shell: "{{ install_dir }}/bin/kubectl create serviceaccount --namespace kube-system tiller &&
           {{ install_dir }}/bin/kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller"
    ignore_errors: yes
    changed_when: false

  - name: Initialize helm
    command: "{{ install_dir }}/bin/helm init --service-account=tiller --history-max 5 --upgrade"
    environment: "{{ proxy_env }}"
    changed_when: false

# Configure sensu cluster metrics
- hosts: masters:workers
  user: "{{ remote_user }}"
  become: yes

  tasks:
  - name: Ensure sensu monitoring script folder exists
    file:
      state: directory
      path: /etc/sensu/plugins/metrics/nais
    when: sensu_rpm_check.rc == 0

  - name: Ensure sensu monitoring scripts exist
    copy:
      src: "files/sensu/checks/{{ item }}"
      dest: "/etc/sensu/plugins/metrics/nais/{{ item }}"
      mode: 0775
    with_items:
      - aggregate-metrics.sh
      - addon-metrics.sh
      - component-metrics.sh
      - interface-metrics.sh
      - nodes-metrics.sh
      - process-metrics.sh
    when: sensu_rpm_check.rc == 0
    notify:
      - restart_sensu

  handlers:
  - name: restart_sensu
    systemd:
      daemon_reload: yes
      name: sensu-client.service
      state: restarted

- hosts: masters
  user: "{{ remote_user }}"
  become: yes

  tasks:
  - name: Add puppet facts
    template:
      src: templates/puppet-facts.master.j2
      dest: /etc/puppetlabs/facter/facts.d/nais.yaml
    when: puppet_agent_rpm_check.rc == 0

  - name: Copy sensu client.json with correct config
    template:
      src: templates/sensu/master-client.json.j2
      dest: /etc/sensu/conf.d/client.json
    when: sensu_rpm_check.rc == 0
    notify:
      - restart_sensu

  - name: Ensure sensu monitoring config for master node exists
    copy:
      src: files/sensu/config/nais_master_metrics.json
      dest: /etc/sensu/conf.d/nais_master_metrics.json
    when: sensu_rpm_check.rc == 0
    notify:
      - restart_sensu

  handlers:
   - name: restart_sensu
     systemd:
      daemon_reload: yes
      name: sensu-client.service
      state: restarted

- hosts: workers
  user: "{{ remote_user }}"
  become: yes

  tasks:
  - name: Add puppet facts
    template:
      src: templates/puppet-facts.worker.j2
      dest: /etc/puppetlabs/facter/facts.d/nais.yaml
    when: puppet_agent_rpm_check.rc == 0

  - name: Copy sensu client.json with correct config
    template:
      src: templates/sensu/worker-client.json.j2
      dest: /etc/sensu/conf.d/client.json
    when: sensu_rpm_check.rc == 0
    notify:
      - restart_sensu

  - name: Ensure sensu monitoring config for worker nodes exist
    copy:
      src: files/sensu/config/nais_worker_metrics.json
      dest: /etc/sensu/conf.d/nais_worker_metrics.json
    when: sensu_rpm_check.rc == 0
    notify:
      - restart_sensu

  handlers:
  - name: restart_sensu
    systemd:
      daemon_reload: yes
      name: sensu-client.service
      state: restarted
