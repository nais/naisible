- hosts: all
  gather_facts: False
  user: "{{ remote_user }}"
  any_errors_fatal: true
  roles:
    - coreos_bootstrap
  pre_tasks:
  - name: Copy python interpreter to server
    delegate_to: localhost
    command: "scp -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null {{ lookup('env', 'SSH_OPTS') }} files/pypy-5.1.0-linux64.tar.bz2 {{ remote_user }}@{{ inventory_hostname }}:/home/{{ remote_user }}/"
    changed_when: false

- hosts: localhost
  tasks:
  - name: Verifiy that facts have been gathered for all hosts in inventory
    assert:
      that:
        - ansible_play_hosts == ansible_play_hosts_all

- hosts: all
  user: "{{ remote_user }}"
  become: yes
  roles:
  -  { role: proxy, when: nais_http_proxy is defined }
  tasks:
  - name: Add systemd unit to fix issue 32728 (symlinks in /sys)
    copy:
      src: files/kube-issue-32728-fix.service
      dest: /etc/systemd/system/kube-issue-32728-fix.service
      owner: root
      group: root
      mode: 0644

  - name: Enable the fix for 32728 during boot
    systemd:
      daemon_reload: yes
      name: kube-issue-32728-fix.service
      state: started
      enabled: yes

- hosts: masters:workers
  user: "{{ remote_user }}"
  become: yes
  roles:
  - k8s_common
  tasks:
  - name: Copy .bashrc
    copy:
      src: files/bashrc
      dest: /root/.bashrc

- hosts: masters
  user: "{{ remote_user }}"
  roles:
  - fetch_existing_certificates

- hosts: masters:workers
  user: "{{ remote_user }}"
  tasks:
  - name: Fetch Kubelet-specific certificates (if exists)
    include_role:
      name: fetch_existing_certificates
      tasks_from: kubelet

- hosts: etcd
  user: "{{ remote_user }}"
  tasks:
  - name: Fetch Etcd-specific certificates (if exists)
    include_role:
      name: fetch_existing_certificates
      tasks_from: etcd
    tags:
      - fetch_etcd_certs

- hosts: localhost
  roles:
  - create_certificates

- hosts: etcd
  user: "{{ remote_user }}"
  become: yes
  roles:
    - etcd

- hosts: etcd[0]
  user: "{{ remote_user }}"
  become: yes
  tasks:
  - name: Check if flannel config exists (OK if this fails)
    command: "{{ install_dir }}/bin/etcdctl ls /nais/network/config"
    environment:
      - ETCDCTL_CERT_FILE: /etc/ssl/etcd/etcd.pem
      - ETCDCTL_KEY_FILE: /etc/ssl/etcd/etcd-key.pem
      - ETCDCTL_CA_FILE: /etc/ssl/etcd/ca.pem
    register: flannelconfig
    ignore_errors: yes
    changed_when: false

  - name: Set flannel configuration in etcd
    shell: '"{{ install_dir }}"/bin/etcdctl set /nais/network/config  "{ \"Network\": \"{{ pod_network_cidr }}\", \"SubnetLen\": 23, \"Backend\": { \"Type\": \"vxlan\" }}"'
    environment:
      - ETCDCTL_CERT_FILE: /etc/ssl/etcd/etcd.pem
      - ETCDCTL_KEY_FILE: /etc/ssl/etcd/etcd-key.pem
      - ETCDCTL_CA_FILE: /etc/ssl/etcd/ca.pem
    when: flannelconfig.rc != 0

# Configure kubernetes Master node
- hosts: masters
  user: "{{ remote_user }}"
  become: yes
  tasks:
  - name: Copy certificates
    copy:
      src: "target/pki/{{ item }}"
      dest: /etc/kubernetes/pki/
    loop:
      - admin.pem
      - admin-key.pem
      - ca.pem
      - ca-key.pem
      - front-proxy-ca.pem
      - front-proxy-ca-key.pem
      - front-proxy-client.pem
      - front-proxy-client-key.pem
      - kube-apiserver-server-key.pem
      - kube-apiserver-server.pem
      - kube-proxy.pem
      - kube-proxy-key.pem
      - sa.key
      - sa.pub
      - traefik.pem
      - traefik-key.pem

  - name: Copy kubelet certificates
    copy:
      src: "target/pki/kubelet/{{ ansible_hostname.split('.')[0] }}{{ item }}"
      dest: "/etc/kubernetes/pki/kubelet{{ item }}"
    loop:
      - ".pem"
      - "-key.pem"

- hosts: masters:workers
  user: "{{ remote_user }}"
  become: yes
  tasks:

  - name: Copy etcd certificates
    copy:
      src: "target/pki/{{ item }}"
      dest: /etc/kubernetes/pki/ # to ensure idempotency
    loop:
      - etcd/etcd.pem
      - etcd/etcd-key.pem
      - ca.pem
    notify:
      - restart_kubelet
      - restart_kubeproxy
      - restart_flannel

  - name: Download flannel binaries # using curl as get_url gave protocol error (most likely caused by internal webproxy)
    environment: "{{ proxy_env }}"
    shell: curl -L --insecure https://github.com/coreos/flannel/releases/download/v{{ flannel_version }}/flannel-v{{ flannel_version }}-linux-amd64.tar.gz | tar xzf - -C /tmp flanneld mk-docker-opts.sh && \
           mv /tmp/flanneld {{ install_dir }}/bin/flanneld-{{ flannel_version }} && \
           mv /tmp/mk-docker-opts.sh {{ install_dir }}/libexec
    args:
      creates: "{{ install_dir }}/bin/flanneld-{{ flannel_version }}"

  - name: Symlink flannel
    file:
      src: "{{ install_dir }}/bin/flanneld-{{ flannel_version }}"
      dest: "{{ install_dir }}/bin/flanneld"
      state: link

  - name: Copy flannel service file
    template:
      src: templates/flannel.service.j2
      dest: /etc/systemd/system/flannel.service
    notify:
      - restart_flannel

  - name: Enable flannel
    systemd:
      daemon_reload: yes
      name: flannel.service
      state: started
      enabled: yes

  - name: Ensure /var/lib/docker exists
    file:
      path: /var/lib/docker
      state: directory

  - name: Force delete docker_opts.env
    file:
      path: /var/lib/docker/docker_opts.env
      state: absent

  - name: Wait for flannel to write subnet and mtu values
    wait_for:
      path: /run/flannel/subnet.env
      state: present
      timeout: 60
      search_regex: FLANNEL_SUBNET
      msg: Timeout waiting for flannel to write subnet and mtu values

  - name: Make flannel configuration for docker
    command: "{{ install_dir }}/libexec/mk-docker-opts.sh -k DOCKER_OPTS -f /run/flannel/subnet.env -d /var/lib/docker/docker_opts.env"
    changed_when: false

  - name: Copy docker service file
    copy:
      src: files/docker.service
      dest: /etc/systemd/system/docker.service
    notify:
      - restart_docker

  - name: Setup docker daemon
    copy:
      src: files/daemon.json
      dest: /etc/docker/daemon.json
    notify:
      - restart_docker

  - name: Start and enable docker
    systemd:
      daemon_reload: yes
      name: docker.service
      state: started
      enabled: yes

  - name: Ensure /root/.docker exists
    file:
      path: /root/.docker
      state: directory

  - name: Add docker credentials if defined
    template:
      src: templates/docker-config.json.j2
      dest: /root/.docker/config.json
    when: docker_repo_url is defined

  - name: Disable swap automount
    mount:
      path: swap
      state: absent

  - name: Disable swap
    command: swapoff -a
    changed_when: false

  - name: Download kubernetes node binaries # using curl as get_url gave protocol error (most likely caused by internal webproxy)
    environment: "{{ proxy_env }}"
    shell: curl -L --insecure https://dl.k8s.io/v{{ k8s_version }}/kubernetes-node-linux-amd64.tar.gz | \
           tar xzf - -C /tmp kubernetes/node/bin/kubectl kubernetes/node/bin/kube-proxy kubernetes/node/bin/kubelet && \
           mv /tmp/kubernetes/node/bin/kube-proxy {{ install_dir }}/bin/kube-proxy-{{ k8s_version }} && \
           mv /tmp/kubernetes/node/bin/kubectl {{ install_dir }}/bin/kubectl-{{ k8s_version }} && \
           mv /tmp/kubernetes/node/bin/kubelet {{ install_dir }}/bin/kubelet-{{ k8s_version }}
    args:
      creates: "{{ install_dir }}/bin/kubelet-{{ k8s_version }}"

  - name: Symlink kubernetes node binaries
    file:
      src: "{{ install_dir }}/bin/{{ item }}-{{ k8s_version }}"
      dest: "{{ install_dir }}/bin/{{ item }}"
      state: link
      force: yes
    loop:
      - kubectl
      - kube-proxy
      - kubelet
    notify:
      - restart_kubelet
      - restart_kubeproxy

  - name: Make kubernetes node binaries executable
    file:
      path: "{{ install_dir }}/bin/{{ item }}-{{ k8s_version }}"
      mode: 0755
    loop:
      - kubectl
      - kube-proxy
      - kubelet

  - name: Copy kube-proxy service file
    template:
      src: templates/kube-proxy.service.j2
      dest: /etc/systemd/system/kube-proxy.service
    notify:
      - restart_kubeproxy

  - name: Copy kube-proxy kubeconfig
    template:
      src: templates/kubeconfigs/kube-proxy.conf.j2
      dest: /etc/kubernetes/kubeconfigs/kube-proxy.conf
    notify:
      - restart_kubeproxy

  - name: Enable kube-proxy
    systemd:
      daemon_reload: yes
      name: kube-proxy.service
      state: started
      enabled: yes

  - name: Set max_user_watches kernel parameter
    sysctl:
      name: fs.inotify.max_user_watches
      value: 16384
      state: present
      sysctl_file: /etc/sysctl.d/60-sysctl.conf
      sysctl_set: yes
      reload: yes

  - name: Set max_map_count kernel parameter
    sysctl:
      name: vm.max_map_count
      value: 262144
      state: present
      sysctl_file: /etc/sysctl.d/60-sysctl.conf
      sysctl_set: yes
      reload: yes

  handlers:
    - name: restart_docker
      systemd:
        daemon_reload: yes
        name: docker.service
        state: restarted

    - name: restart_kubelet
      systemd:
        daemon_reload: yes
        name: kubelet.service
        state: restarted
      # on new nodes kublet will not yet be install, ignore errors
      ignore_errors: yes

    - name: restart_flannel
      systemd:
        daemon_reload: yes
        name: flannel.service
        state: restarted

    - name: restart_kubeproxy
      systemd:
        daemon_reload: yes
        name: kube-proxy.service
        state: restarted

# Start control plane
- hosts: masters
  user: "{{ remote_user }}"
  become: yes
  tasks:

  - name: Copy kubelet service manifest
    template:
      src: templates/master-kubelet.service.j2
      dest: /etc/systemd/system/kubelet.service
    notify:
      - restart_kubelet

  - name: Copy kubelet kubeconfig
    template:
      src: templates/kubeconfigs/master-kubelet.conf.j2
      dest: /etc/kubernetes/kubeconfigs/kubelet.conf
    notify:
      - restart_kubelet

  - name: Enable kubelet
    systemd:
      daemon_reload: yes
      name: kubelet.service
      state: started
      enabled: yes

  - name: Ensure kubectl config directory exists on master
    file:
      state: directory
      path: /root/.kube

  - name: Copy API server pod description
    copy:
      src: files/kubectl-config
      dest: /root/.kube/config

  - name: Ensure manifests directory exists on master
    file:
      state: directory
      path: /etc/kubernetes/manifests

  - name: Copy manifests and kubeconfigs
    template:
      src: "templates/{{ item }}.j2"
      dest: "/etc/kubernetes/{{ item }}"
    tags:
    - devel
    loop:
      - kube-apiserver-audit-policy.yaml
      - manifests/kube-apiserver.yaml
      - manifests/kube-scheduler.yaml
      - manifests/kube-controller-manager.yaml
      - kubeconfigs/kube-scheduler.conf
      - kubeconfigs/kube-controller-manager.conf
    notify:
      - restart_docker

  handlers:
    - name: restart_docker
      systemd:
        daemon_reload: yes
        name: docker.service
        state: restarted

    - name: restart_kubelet
      systemd:
        daemon_reload: yes
        name: kubelet.service
        state: restarted

# Configure workers
- hosts: workers
  user: "{{ remote_user }}"
  become: yes
  tasks:
  - name: Copy kubelet certificates
    copy:
      src: "target/pki/kubelet/{{ ansible_hostname.split('.')[0] }}{{ item }}"
      dest: "/etc/kubernetes/pki/kubelet{{ item }}"
    loop:
      - ".pem"
      - "-key.pem"
    notify:
      - restart_kubelet

  - name: Copy kube-proxy certificates
    copy:
      src: "target/pki/{{ item }}"
      dest: /etc/kubernetes/pki/
    loop:
      - admin.pem
      - admin-key.pem
      - kube-proxy.pem
      - kube-proxy-key.pem
    notify:
      - restart_kubeproxy

  - name: Copy Issuing Internal certificate
    copy:
      src: files/NAVIssuingCAIntern.crt
      dest: "{{ cert_dir }}/NAV_Issuing_Intern_CA{{ cert_postfix }}"
      force: no

  - name: Ensure certs.d directory exists worker nodes
    file:
      state: directory
      path: /etc/docker/certs.d/docker.adeo.no:5000/

  - name: Symlink Internal CA to docker
    file:
      src: "/etc/ssl/certs/NAV_Issuing_Intern_CA{{ cert_postfix }}"
      dest: /etc/docker/certs.d/docker.adeo.no:5000/ca.crt
      state: link
      force: yes

  - name: Copy kubelet service manifest
    template:
      src: templates/worker-kubelet.service.j2
      dest: /etc/systemd/system/kubelet.service
    notify:
      - restart_kubelet

  - name: Copy kubelet kubeconfig
    template:
      src: templates/kubeconfigs/worker-kubelet.conf.j2
      dest: /etc/kubernetes/kubeconfigs/kubelet.conf
    notify:
      - restart_kubelet

  - name: Enable kubelet
    systemd:
      daemon_reload: yes
      name: kubelet.service
      state: started
      enabled: yes

  handlers:
    - name: restart_kubeproxy
      systemd:
        daemon_reload: yes
        name: kube-proxy.service
        state: restarted

    - name: restart_kubelet
      systemd:
        daemon_reload: yes
        name: kubelet.service
        state: restarted

- hosts: masters
  user: "{{ remote_user }}"
  become: yes
  tasks:
    - name: Wait for API server to become available
      wait_for:
        port: 6443
        host: "{{ groups.masters|random }}"
        delay: 15
        timeout: 240
      changed_when: false

- hosts: masters:workers
  user: "{{ remote_user }}"
  become: yes
  roles:
    - generate_kubeconfigs
  tasks:
    - name: Setup node taints
      command: "{{ install_dir }}/bin/kubectl --kubeconfig=/etc/kubernetes/kubeconfigs/kubelet.conf  --overwrite=true taint nodes {{ inventory_hostname }} {{ item }}"
      loop: "{{ node_taints|default([]) }}"
      changed_when: false

    - name: Setup node labels
      command: "{{ install_dir }}/bin/kubectl --kubeconfig=/etc/kubernetes/kubeconfigs/kubelet.conf --overwrite=true label nodes {{ inventory_hostname }} {{ item }}"
      loop: "{{ node_labels|default([]) }}"
      changed_when: false

    # Denne blir fjernet etter nytt oppsett har blitt kjørt på alle noder
    - name: Delete cluster-admin.conf
      file:
        path: /etc/kubernetes/kubeconfigs/cluster-admin.conf
        state: absent
      changed_when: false

- hosts: masters
  user: "{{ remote_user }}"
  become: yes

  tasks:
  - name: Ensure addons directory exists
    file:
      state: directory
      path: /etc/kubernetes/addons

  - name: Ensure roles directory exists
    file:
      state: directory
      path: /etc/kubernetes/roles

  - name: Copy kubernetes-addons manifest templates
    template:
      src: "templates/addons/{{ item }}.j2"
      dest: "/etc/kubernetes/addons/{{ item }}"
    loop:
      - core-dns.yaml
      - traefik.yaml
      - heapster.yaml
      - nais.yaml
      - debug.yaml


  - name: Copy kubernetes clusterrolebindings
    copy:
      src: "files/roles/{{ item }}"
      dest: "/etc/kubernetes/roles/{{ item }}"
    loop:
      - clusterrolebindings.yaml

  - name: Wait for API server to become available
    wait_for:
      port: 8080
      host: 127.0.0.1
      delay: 5
    changed_when: false

  - name: Create clusterrole and clusterrolebindings
    command: "{{ install_dir }}/bin/kubectl apply -f /etc/kubernetes/roles/{{ item }}"
    loop:
    - clusterrolebindings.yaml
    changed_when: false

  - name: Create traefik tls secret
    command: "{{ install_dir }}/bin/kubectl -n kube-system create secret tls traefik-cert --key=/etc/kubernetes/pki/traefik-key.pem --cert=/etc/kubernetes/pki/traefik.pem"
    ignore_errors: yes
    changed_when: false

  - name: Create addons
    command: "{{ install_dir }}/bin/kubectl apply -f /etc/kubernetes/addons/{{ item }}"
    loop:
      - core-dns.yaml
      - traefik.yaml
      - heapster.yaml
      - nais.yaml
      - debug.yaml
    changed_when: false

  - name: Download helm binaries # using curl as get_url gave protocol error (most likely caused by internal webproxy)
    environment: "{{ proxy_env }}"
    shell: curl -L --insecure https://storage.googleapis.com/kubernetes-helm/helm-v{{ helm_version }}-linux-amd64.tar.gz | \
           tar xzf - -C /tmp linux-amd64/helm && \
           mv /tmp/linux-amd64/helm {{ install_dir }}/bin/helm-{{ helm_version }}
    args:
      creates: "{{ install_dir }}/bin/helm-{{ helm_version }}"

  - name: Symlink helm
    file:
      src: "{{ install_dir }}/bin/helm-{{ helm_version }}"
      dest: "{{ install_dir }}/bin/helm"
      state: link

  - name: Setup helm service account
    shell: "{{ install_dir }}/bin/kubectl create serviceaccount --namespace kube-system tiller &&
           {{ install_dir }}/bin/kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller"
    ignore_errors: yes
    changed_when: false

  - name: Initialize helm
    command: "{{ install_dir }}/bin/helm init --service-account=tiller --history-max 5 --upgrade"
    environment: "{{ proxy_env }}"
    changed_when: false

