- hosts: all
  gather_facts: False
  user: "{{ remote_user }}"
  any_errors_fatal: true
  roles:
    - coreos_bootstrap
  pre_tasks:
  - name: Copy python interpreter to server
    delegate_to: localhost
    command: "scp -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null {{ lookup('env', 'SSH_OPTS') }} files/pypy2.7-v7.3.1-linux64.tar.bz2 {{ remote_user }}@{{ inventory_hostname }}:/home/{{ remote_user }}/"
    changed_when: false

- hosts: localhost
  tasks:
  - name: Verifiy that facts have been gathered for all hosts in inventory
    assert:
      that:
        - ansible_play_hosts == ansible_play_hosts_all

- hosts: masters:workers:ceph_nodes
  user: "{{ remote_user }}"
  become: yes
  tasks:
  - name: Ensure locksmithd is stopped
    systemd:
      daemon_reload: yes
      name: locksmithd.service
      state: stopped
      masked: yes
    ignore_errors: yes

- hosts: etcd
  user: "{{ remote_user }}"
  become: yes
  tasks:
    - name: Copy update.conf to etcd nodes
      copy:
        src: files/etcd/etcd_update.conf
        dest: /etc/coreos/update.conf

    - name: Copy backup script to etcd nodes
      copy:
        src: files/etcd/etcd_backup.sh
        dest: "/home/{{ remote_user }}/etcd_backup.sh"
        mode: 0744

    - name: Copy etcd backup service and timer to etcd nodes
      copy:
        src: files/etcd/{{ item }}
        dest: /etc/systemd/system/
      loop:
        - etcd-backup.service
        - etcd-backup.timer
        - etcd-prometheus-push.service
        - etcd-prometheus-push.timer

    - name: Start and enable etcd-backend.timer
      systemd:
        daemon_reload: yes
        name: "{{ item }}"
        state: started
        enabled: yes
      loop:
        - etcd-backup.timer
        - etcd-prometheus-push.timer

- hosts: etcd:prometheus:githubrunner
  user: "{{ remote_user }}"
  become: yes
  tasks:
    - name: Ensure locksmithd is started
      systemd:
        daemon_reload: yes
        name: locksmithd.service
        state: started
        enabled: yes

- hosts: all
  user: "{{ remote_user }}"
  become: yes
  roles:
  -  { role: proxy, when: nais_http_proxy is defined }
  - os-users
  - logs
  - update-engine
  - node_exporter
  tasks:
  - name: Add systemd unit to fix issue 32728 (symlinks in /sys)
    copy:
      src: files/kube-issue-32728-fix.service
      dest: /etc/systemd/system/kube-issue-32728-fix.service
      owner: root
      group: root
      mode: 0644

  - name: Enable the fix for 32728 during boot
    systemd:
      daemon_reload: yes
      name: kube-issue-32728-fix.service
      state: started
      enabled: yes

- hosts: prometheus
  gather_facts: true
  user: "{{ remote_user }}"
  become: yes
  any_errors_fatal: true
  roles:
    - prometheus

- hosts: masters:workers
  user: "{{ remote_user }}"
  become: yes
  roles:
  - k8s_common
  tasks:
  - name: Remove /root/.bashrc
    file:
      path: /root/.bashrc
      state: absent

- hosts: masters
  user: "{{ remote_user }}"
  roles:
  - fetch_existing_certificates

- hosts: masters:workers
  user: "{{ remote_user }}"
  tasks:
  - name: Fetch Kubelet-specific certificates (if exists)
    include_role:
      name: fetch_existing_certificates
      tasks_from: kubelet

- hosts: etcd
  user: "{{ remote_user }}"
  tasks:
  - name: Fetch Etcd-specific certificates (if exists)
    include_role:
      name: fetch_existing_certificates
      tasks_from: etcd
    tags:
      - fetch_etcd_certs

- hosts: localhost
  roles:
  - create_certificates

- hosts: etcd
  user: "{{ remote_user }}"
  become: yes
  roles:
    - etcd

- hosts: etcd[0]
  user: "{{ remote_user }}"
  become: yes
  tasks:
  - name: Check if flannel config exists (OK if this fails)
    command: "{{ install_dir }}/bin/etcdctl ls /nais/network/config"
    environment:
      - ETCDCTL_CERT_FILE: /etc/ssl/etcd/etcd-client.pem
      - ETCDCTL_KEY_FILE: /etc/ssl/etcd/etcd-client-key.pem
      - ETCDCTL_CA_FILE: /etc/ssl/etcd/ca.pem
    register: flannelconfig
    ignore_errors: yes
    changed_when: false

  - name: Set flannel configuration in etcd
    shell: '"{{ install_dir }}"/bin/etcdctl set /nais/network/config  "{ \"Network\": \"{{ pod_network_cidr }}\", \"SubnetLen\": 23, \"Backend\": { \"Type\": \"vxlan\" }}"'
    environment:
      - ETCDCTL_CERT_FILE: /etc/ssl/etcd/etcd-client.pem
      - ETCDCTL_KEY_FILE: /etc/ssl/etcd/etcd-client-key.pem
      - ETCDCTL_CA_FILE: /etc/ssl/etcd/ca.pem
    when: flannelconfig.rc != 0

# Configure kubernetes Master node
- hosts: masters
  user: "{{ remote_user }}"
  become: yes
  tasks:
  - name: Copy certificates
    copy:
      src: "target/pki/{{ item }}"
      dest: /etc/kubernetes/pki/
    loop:
      - admin.pem
      - admin-key.pem
      - ca.pem
      - ca-key.pem
      - front-proxy-ca.pem
      - front-proxy-ca-key.pem
      - front-proxy-client.pem
      - front-proxy-client-key.pem
      - kube-apiserver-server-key.pem
      - kube-apiserver-server.pem
      - kube-proxy.pem
      - kube-proxy-key.pem
      - prometheus-adapter.pem
      - prometheus-adapter-key.pem
      - sa.key
      - sa.pub
      - traefik.pem
      - traefik-key.pem

  - name: Copy kubelet certificates
    copy:
      src: "target/pki/kubelet/{{ ansible_hostname.split('.')[0] }}{{ item }}"
      dest: "/etc/kubernetes/pki/kubelet{{ item }}"
    loop:
      - ".pem"
      - "-key.pem"

- hosts: masters:workers
  user: "{{ remote_user }}"
  become: yes
  tasks:

  - name: Copy etcd certificates
    copy:
      src: "target/pki/{{ item }}"
      dest: /etc/kubernetes/pki/ # to ensure idempotency
    loop:
      - etcd/etcd-client.pem
      - etcd/etcd-client-key.pem
      - ca.pem
    notify:
      - restart_kubelet
      - restart_kubeproxy

  - name: Copy etcd certificates to default cert directory
    copy:
      src: "target/pki/{{ item }}"
      dest: /etc/ssl/certs/ # bundled flannel needs certificates here
    loop:
      - etcd/etcd-client.pem
      - etcd/etcd-client-key.pem
      - ca.pem
    notify:
      - restart_flannel

  - name: Copy mk-docker-opts file
    template:
      src: files/mk-docker-opts.sh
      dest: "{{ install_dir }}/libexec/mk-docker-opts.sh"

  - name: Copy flannel service file
    template:
      src: templates/flannel.service.j2
      dest: /etc/systemd/system/flannel.service
    notify:
      - restart_flannel

  - name: Enable flannel
    systemd:
      daemon_reload: yes
      name: flannel.service
      state: started
      enabled: yes

  - name: Ensure /var/lib/docker exists
    file:
      path: /var/lib/docker
      state: directory

  - name: Force delete docker_opts.env
    file:
      path: /var/lib/docker/docker_opts.env
      state: absent

  - name: Wait for flannel to write subnet and mtu values
    wait_for:
      path: /run/flannel/subnet.env
      state: present
      timeout: 60
      search_regex: FLANNEL_SUBNET
      msg: Timeout waiting for flannel to write subnet and mtu values

  - name: Make flannel configuration for docker
    command: "{{ install_dir }}/libexec/mk-docker-opts.sh -k DOCKER_OPTS -f /run/flannel/subnet.env -d /var/lib/docker/docker_opts.env"
    changed_when: false

  - name: Copy docker service file
    copy:
      src: files/docker.service
      dest: /etc/systemd/system/docker.service
    notify:
      - restart_docker

  - name: Setup docker daemon
    copy:
      src: files/daemon.json
      dest: /etc/docker/daemon.json
    notify:
      - restart_docker

  - name: Start and enable docker
    systemd:
      daemon_reload: yes
      name: docker.service
      state: started
      enabled: yes

  - name: Ensure /root/.docker exists
    file:
      path: /root/.docker
      state: directory

  - name: Add docker credentials if defined
    template:
      src: templates/docker-config.json.j2
      dest: /root/.docker/config.json
    when: docker_repo_url is defined

  - name: Disable swap automount
    mount:
      path: swap
      state: absent

  - name: Disable swap
    command: swapoff -a
    changed_when: false

  - name: Download kubernetes proxy 1.15.4 binary # Remove after network issue with 1.16 is resolved.
    environment: "{{ proxy_env }}"
    shell: curl -L --insecure https://dl.k8s.io/v{{ kube_proxy_version }}/kubernetes-node-linux-amd64.tar.gz | \
           tar xzf - -C /tmp kubernetes/node/bin/kube-proxy && \
           mv /tmp/kubernetes/node/bin/kube-proxy {{ install_dir }}/bin/kube-proxy-{{ kube_proxy_version }}
    args:
      creates: "{{ install_dir }}/bin/kube-proxy-{{ kube_proxy_version }}"

  - name: Download kubernetes node binaries # using curl as get_url gave protocol error (most likely caused by internal webproxy)
    environment: "{{ proxy_env }}"
    shell: curl -L --insecure https://dl.k8s.io/v{{ k8s_version }}/kubernetes-node-linux-amd64.tar.gz | \
           tar xzf - -C /tmp kubernetes/node/bin/kubectl kubernetes/node/bin/kube-proxy kubernetes/node/bin/kubelet && \
           mv /tmp/kubernetes/node/bin/kube-proxy {{ install_dir }}/bin/kube-proxy-{{ k8s_version }} && \
           mv /tmp/kubernetes/node/bin/kubectl {{ install_dir }}/bin/kubectl-{{ k8s_version }} && \
           mv /tmp/kubernetes/node/bin/kubelet {{ install_dir }}/bin/kubelet-{{ k8s_version }}
    args:
      creates: "{{ install_dir }}/bin/kubelet-{{ k8s_version }}"

  - name: Symlink kubernetes node binaries
    file:
      src: "{{ install_dir }}/bin/{{ item }}-{{ k8s_version }}"
      dest: "{{ install_dir }}/bin/{{ item }}"
      state: link
      force: yes
    loop:
      - kubectl
      - kube-proxy
      - kubelet
    notify:
      - restart_kubelet
      - restart_kubeproxy

  - name: Make kubernetes node binaries executable
    file:
      path: "{{ install_dir }}/bin/{{ item }}-{{ k8s_version }}"
      mode: 0755
    loop:
      - kubectl
      - kube-proxy
      - kubelet

  - name: Install conntrack binary
    copy:
      src: files/bin/conntrack
      dest: /opt/bin/conntrack
      mode: 0755

  - name: Copy kube-proxy service file
    template:
      src: templates/kube-proxy.service.j2
      dest: /etc/systemd/system/kube-proxy.service
    notify:
      - restart_kubeproxy

  - name: Enable kube-proxy
    systemd:
      daemon_reload: yes
      name: kube-proxy.service
      state: started
      enabled: yes

  - name: Set max_user_watches kernel parameter
    sysctl:
      name: fs.inotify.max_user_watches
      value: 16384
      state: present
      sysctl_file: /etc/sysctl.d/60-sysctl.conf
      sysctl_set: yes
      reload: yes

  - name: Set max_map_count kernel parameter
    sysctl:
      name: vm.max_map_count
      value: 262144
      state: present
      sysctl_file: /etc/sysctl.d/60-sysctl.conf
      sysctl_set: yes
      reload: yes

  handlers:
    - name: restart_docker
      systemd:
        daemon_reload: yes
        name: docker.service
        state: restarted

    - name: restart_kubelet
      systemd:
        daemon_reload: yes
        name: kubelet.service
        state: restarted
      # on new nodes kublet will not yet be install, ignore errors
      ignore_errors: yes

    - name: restart_flannel
      systemd:
        daemon_reload: yes
        name: flannel.service
        state: restarted

    - name: restart_kubeproxy
      systemd:
        daemon_reload: yes
        name: kube-proxy.service
        state: restarted

# Start control plane
- hosts: masters
  user: "{{ remote_user }}"
  become: yes
  tasks:

  - name: Copy kubelet master config
    copy:
      src: files/kubelet-master-config.yaml
      dest: /etc/kubernetes/kubeconfigs/kubelet-config.yaml
    notify:
      - restart_kubelet

  - name: Copy kubelet service manifest
    template:
      src: templates/master-kubelet.service.j2
      dest: /etc/systemd/system/kubelet.service
    notify:
      - restart_kubelet

  - name: Copy kubelet kubeconfig
    template:
      src: templates/kubeconfigs/master-kubelet.conf.j2
      dest: /etc/kubernetes/kubeconfigs/kubelet.conf
    notify:
      - restart_kubelet

  - name: Enable kubelet
    systemd:
      daemon_reload: yes
      name: kubelet.service
      state: started
      enabled: yes

  - name: Ensure kubectl config directory exists on master
    file:
      state: directory
      path: /root/.kube

  - name: Copy API server pod description
    copy:
      src: files/kubectl-config
      dest: /root/.kube/config

  - name: Ensure manifests directory exists on master
    file:
      state: directory
      path: /etc/kubernetes/manifests

  - name: Copy manifests and kubeconfigs
    template:
      src: "templates/{{ item }}.j2"
      dest: "/etc/kubernetes/{{ item }}"
    tags:
    - devel
    loop:
      - kube-apiserver-audit-policy.yaml
      - manifests/kube-apiserver.yaml
      - manifests/kube-scheduler.yaml
      - manifests/kube-controller-manager.yaml
      - kubeconfigs/kube-scheduler.conf
      - kubeconfigs/kube-controller-manager.conf
    notify:
      - restart_docker

  handlers:
    - name: restart_docker
      systemd:
        daemon_reload: yes
        name: docker.service
        state: restarted

    - name: restart_kubelet
      systemd:
        daemon_reload: yes
        name: kubelet.service
        state: restarted

# Configure workers
- hosts: workers
  user: "{{ remote_user }}"
  become: yes
  tasks:
  - name: Copy kubelet certificates
    copy:
      src: "target/pki/kubelet/{{ ansible_hostname.split('.')[0] }}{{ item }}"
      dest: "/etc/kubernetes/pki/kubelet{{ item }}"
    loop:
      - ".pem"
      - "-key.pem"
    notify:
      - restart_kubelet

  - name: Copy kube-proxy certificates
    copy:
      src: "target/pki/{{ item }}"
      dest: /etc/kubernetes/pki/
    loop:
      - kube-proxy.pem
      - kube-proxy-key.pem
    notify:
      - restart_kubeproxy

  - name: Copy Issuing Internal certificate
    copy:
      src: files/NAVIssuingCAIntern.crt
      dest: "{{ cert_dir }}/NAV_Issuing_Intern_CA{{ cert_postfix }}"
      force: no

  - name: Ensure certs.d directory exists worker nodes
    file:
      state: directory
      path: /etc/docker/certs.d/docker.adeo.no:5000/

  - name: Symlink Internal CA to docker
    file:
      src: "/etc/ssl/certs/NAV_Issuing_Intern_CA{{ cert_postfix }}"
      dest: /etc/docker/certs.d/docker.adeo.no:5000/ca.crt
      state: link
      force: yes

  - name: Copy kubelet worker config
    template:
      src: templates/kubeconfigs/kubelet-worker-config.yaml.j2
      dest: /etc/kubernetes/kubeconfigs/kubelet-config.yaml
    notify:
      - restart_kubelet

  - name: Copy kubelet service manifest
    template:
      src: templates/worker-kubelet.service.j2
      dest: /etc/systemd/system/kubelet.service
    notify:
      - restart_kubelet

  - name: Copy kubelet kubeconfig
    template:
      src: templates/kubeconfigs/worker-kubelet.conf.j2
      dest: /etc/kubernetes/kubeconfigs/kubelet.conf
    notify:
      - restart_kubelet

  - name: Enable kubelet
    systemd:
      daemon_reload: yes
      name: kubelet.service
      state: started
      enabled: yes

  handlers:
    - name: restart_kubeproxy
      systemd:
        daemon_reload: yes
        name: kube-proxy.service
        state: restarted

    - name: restart_kubelet
      systemd:
        daemon_reload: yes
        name: kubelet.service
        state: restarted

- hosts: masters
  user: "{{ remote_user }}"
  become: yes
  tasks:
    - name: Generate cluster-admin kubeconfig, used by nsync
      shell: "{{ install_dir }}/bin/kubectl config set-cluster {{ cluster_name }} --server=https://{{ groups.masters|first }}:6443 --certificate-authority=/etc/kubernetes/pki/ca.pem --embed-certs=true --kubeconfig=/etc/kubernetes/kubeconfigs/cluster-admin.conf &&
             {{ install_dir }}/bin/kubectl config set-credentials {{ cluster_name }}-cluster-admin --client-certificate=/etc/kubernetes/pki/admin.pem --client-key=/etc/kubernetes/pki/admin-key.pem --embed-certs --kubeconfig=/etc/kubernetes/kubeconfigs/cluster-admin.conf &&
             {{ install_dir }}/bin/kubectl config set-context {{ cluster_name }} --cluster={{ cluster_name }} --user={{ cluster_name }}-cluster-admin --kubeconfig=/etc/kubernetes/kubeconfigs/cluster-admin.conf &&
             {{ install_dir }}/bin/kubectl config use-context {{ cluster_name }} --kubeconfig=/etc/kubernetes/kubeconfigs/cluster-admin.conf"
      changed_when: false

    - name: Wait for API server to become available
      wait_for:
        port: 6443
        host: "{{ groups.masters|first }}"
        delay: 15
        timeout: 240
      changed_when: false

- hosts: masters[0]
  user: "{{ remote_user }}"
  become: yes
  tasks:
    - name: Copy kubeconfig
      fetch:
        src: "/etc/kubernetes/kubeconfigs/cluster-admin.conf"
        dest: "target/kubeconfigs/cluster-admin.conf"
        flat: yes

- hosts: masters:workers
  user: "{{ remote_user }}"
  become: yes
  roles:
    - generate_kubeconfigs
  tasks:
    - name: Setup node taints
      command: "{{ install_dir }}/bin/kubectl --kubeconfig=/etc/kubernetes/kubeconfigs/kubelet.conf  --overwrite=true taint nodes {{ inventory_hostname }} {{ item }}"
      loop: "{{ node_taints|default([]) }}"
      changed_when: false

    - name: Setup node labels
      command: "{{ install_dir }}/bin/kubectl --kubeconfig=/etc/kubernetes/kubeconfigs/kubelet.conf --overwrite=true label nodes {{ inventory_hostname }} {{ item }}"
      loop: "{{ extra_node_labels|default([]) }}"
      changed_when: false

- hosts: masters
  user: "{{ remote_user }}"
  become: yes

  tasks:
  - name: Ensure addons directory exists
    file:
      state: directory
      path: /etc/kubernetes/addons

  - name: Copy kubernetes-addons manifest templates
    template:
      src: "templates/addons/{{ item }}.j2"
      dest: "/etc/kubernetes/addons/{{ item }}"
    loop:
      - core-dns.yaml
      - traefik.yaml
      - prometheus-adapter.yaml
      - nais.yaml
      - debug.yaml

  - name: Copy kubernetes-addons manifests
    copy:
      src: "files/addons/{{ item }}"
      dest: "/etc/kubernetes/addons/{{ item }}"
    loop:
      - clusterrolebindings.yaml
      - psp-kube-system.yaml
      - psp-rbac.yaml

  - name: Wait for API server to become available
    wait_for:
      port: 8080
      host: 127.0.0.1
      delay: 5
    changed_when: false

  - name: Create traefik tls secret
    command: "{{ install_dir }}/bin/kubectl -n kube-system create secret tls traefik-cert --key=/etc/kubernetes/pki/traefik-key.pem --cert=/etc/kubernetes/pki/traefik.pem"
    ignore_errors: yes
    changed_when: false

  - name: Apply addons
    command: "{{ install_dir }}/bin/kubectl apply -f /etc/kubernetes/addons/{{ item }}"
    loop:
      - clusterrolebindings.yaml
      - psp-kube-system.yaml
      - psp-rbac.yaml
      - core-dns.yaml
      - traefik.yaml
      - prometheus-adapter.yaml
      - nais.yaml
      - debug.yaml
    changed_when: false

- hosts: githubrunner
  gather_facts: true
  user: "{{ remote_user }}"
  become: yes
  any_errors_fatal: true
  roles:
    - github-runner
